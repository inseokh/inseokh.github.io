<!DOCTYPE html>

<html>
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title></title>
    <style>
        .footer {
            position: fixed;
            left: 0;
            bottom: 0;
            width: 100%;
            color: grey;
            text-align: left;
            font-size: 0.8rem;
        }
    </style>
</head>
<body>
    <h1>Joyful Life with CSED490D</h1>

    <button id="feelButton" style="height:5em; width:50%">How do I feel?</button>
    <p>Voice sensing time remaining: <span id="time_remain"></span></p>
    <div id="result" style="display:none;">
        <ul>
            <li id="li_error" style="display: none;">Error: <span id="result_error">N/A</span></li>
            <li>Calm: <span id="result_calm">N/A</span></li>
            <li>Anger: <span id="result_anger">N/A</span></li>
            <li>Joy: <span id="result_joy" style='color: red; font-weight: bold'>N/A</span></li>
            <li>Sorrow: <span id="result_sorrow">N/A</span></li>
            <li>Energy: <span id="result_energy">N/A</span></li>
        </ul>
    </div>
    
    <div id="debug_controls" style="display:none;">
        <button id="startRecordingButton">Start recording</button>
        <button id="stopRecordingButton">Stop recording</button>
        <button id="playButton">Play</button>
        <button id="downloadButton">Download</button>
        <button id="emotionButton">Emotion</button>
    </div>

    <div id="footer" class="footer">
        <p>Emotion API powered by <a href="https://webempath.net/lp-eng/">Empath</a> and audio-recording adopted from <a href="https://gist.github.com/meziantou/edb7217fddfbb70e899e">meziantou</a>. Integrated by <a href="https://www.inseokhwang.com/">Inseok Hwang</a> for <a href="https://www.his-lab.org/course-mobile-ubicomp">CSED490D</a> course at <a href="https://ecse.postech.ac.kr/">CSE, POSTECH</a>. <a href="" id='toggleDebug'>Toggle debug controls</a></p>
    </div>

    <script src="jquery-3.6.0.js"></script> 
    <script src="AudioContextMonkeyPatch.js"></script>
    <script>
        var feelButton = document.getElementById("feelButton");
        
        var startRecordingButton = document.getElementById("startRecordingButton");
        var stopRecordingButton = document.getElementById("stopRecordingButton");
        var playButton = document.getElementById("playButton");
        var downloadButton = document.getElementById("downloadButton");
        var emotionButton = document.getElementById("emotionButton");

        var apiKey = 'COzs9xoZWiR-0mO5akDfZHfv86E_PHXZmeFCTewHkZs';
        var apiEndPointV2 = 'https://api.webempath.net/v2/analyzeWav';

        var leftchannel = [];
        var rightchannel = [];
        var recorder = null;
        var recordingLength = 0;
        var volume = null;
        var mediaStream = null;
        var sampleRate = 44100;
        var TARGET_SAMPLE_RATE = 11025;
        var context = null;
        var blob = null;
        var apiReturnData = null;
        var timer_exp = 5000.0;   // 4.5 sec
        var precision = 2;
        $('#time_remain').text((timer_exp/1000).toFixed(precision));

        toggleDebug.addEventListener("click", function(e) {
            e.preventDefault();
            var elem = $('#debug_controls');
            if ( elem.css("display") == "none" ) {
                elem.show();
            }else{
                elem.hide();
            }
        });

        feelButton.addEventListener("click", function(){
            var constraints = {audio: true};
            navigator.mediaDevices.getUserMedia(constraints)
            .then(function(e) {
                startRecording(e);
                var ref = new Date().getTime() + timer_exp;
                timer = setInterval(function() {
                    var now = new Date().getTime();
                    $('#time_remain').text((Math.round(ref-now)/1000).toFixed(precision));

                    if ((ref - now) <= 200) {   // max recording time enforced by API is 5000 msec. Just to prevent slight overflow, less than 200 ms is discarded. 
                        $('#time_remain').text((0).toFixed(precision));
                        clearInterval(timer);
                        stopRecording();
                        getEmotion();
                    }
                }, 100);
            })
            .catch(function(err) {
                console.log('Media permission failed.');
            });

            // navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
            // navigator.getUserMedia(
            //     {
            //         audio: true
            //     },
            //     function (e) {
            //         startRecording(e);
            //         var ref = new Date().getTime() + timer_exp;
            //         timer = setInterval(function() {
            //             var now = new Date().getTime();
            //             $('#time_remain').text((Math.round(ref-now)/1000).toFixed(precision));

            //             if ((ref - now) <= 200) {   // max recording time enforced by API is 5000 msec. Just to prevent slight overflow, less than 200 ms is discarded. 
            //                 $('#time_remain').text((0).toFixed(precision));
            //                 clearInterval(timer);
            //                 stopRecording();
            //                 getEmotion();
            //             }
            //         }, 100);
            //     }, 
            //     function (e) {
            //         console.error(e);
            //     }
            // );
        })

        startRecordingButton.addEventListener("click", startRecordingWithPermission);

        stopRecordingButton.addEventListener("click", stopRecording);

        emotionButton.addEventListener("click", getEmotion);

        playButton.addEventListener("click", function () {
            if (blob == null) {
                return;
            }

            var url = window.URL.createObjectURL(blob);
            var audio = new Audio(url);
            audio.play();
        });

        downloadButton.addEventListener("click", function () {
            if (blob == null) {
                return;
            }

            var url = URL.createObjectURL(blob);

            var a = document.createElement("a");
            document.body.appendChild(a);
            a.style = "display: none";
            a.href = url;
            a.download = "sample.wav";
            a.click();
            window.URL.revokeObjectURL(url);
        });

        function startRecordingWithPermission() {
            // Initialize recorder
            var constraints = {audio: true};
            navigator.mediaDevices.getUserMedia(constraints)
            .then(startRecording)
            .catch(function(err) {
                console.log('Media permission failed.');
            });

            // navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia || navigator.mozGetUserMedia || navigator.msGetUserMedia;
            // navigator.getUserMedia(
            // {
            //     audio: true
            // },
            // startRecording,
            // function (e) {
            //         console.error(e);
            //     });
        }

        function startRecording(e) {
            console.log("user consent");

            // reset audio buffers
            leftchannel = [];
            rightchannel = [];
            recordingLength = 0;

            // creates the audio context
            context = new AudioContext();
            // const AudioContext = window.AudioContext || window.webkitAudioContext;
            // context = new AudioContext();
            // window.AudioContext = window.AudioContext || window.webkitAudioContext;
            // context = new AudioContext();

            // creates an audio node from the microphone incoming stream
            mediaStream = context.createMediaStreamSource(e);

            // https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createScriptProcessor
            // bufferSize: the onaudioprocess event is called when the buffer is full
            var bufferSize = 2048;
            var numberOfInputChannels = 1;
            var numberOfOutputChannels = 1;
            if (context.createScriptProcessor) {
                recorder = context.createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels);
            } else {
                recorder = context.createJavaScriptNode(bufferSize, numberOfInputChannels, numberOfOutputChannels);
            }

            

            recorder.onaudioprocess = function (e) {
                        leftchannel.push(new Float32Array(e.inputBuffer.getChannelData(0)));
                        console.log(e.inputBuffer.getChannelData(0).length);
                        // rightchannel.push(new Float32Array(e.inputBuffer.getChannelData(1)));
                        recordingLength += bufferSize;
                    };

            // recorder.onaudioprocess = function (e) {
            //     var offlineCtx = new OfflineAudioContext(numberOfInputChannels,
            //                                             e.inputBuffer.duration * TARGET_SAMPLE_RATE,
            //                                             TARGET_SAMPLE_RATE);
            //     // Play it from the beginning.
            //     var offlineSource = offlineCtx.createBufferSource();
            //     offlineSource.buffer = e.inputBuffer;
            //     offlineSource.connect(offlineCtx.destination);
            //     offlineSource.start();
            //     offlineCtx.startRendering();
            //     offlineCtx.oncomplete = function(resampled){
            //     // `resampled` contains an AudioBuffer resampled at 16000Hz.
            //     // use resampled.getChannelData(x) to get an Float32Array for channel x.
            //         leftchannel.push(new Float32Array(resampled.renderedBuffer));
            //         console.log(resampled);
            //         console.log(resampled.renderedBuffer);
            //         // leftchannel.push(new Float32Array(resampled.getChannelData(0)));

            //         // rightchannel.push(new Float32Array(resampled.getChannelData(1)));    // using mono channel
            //         // recordingLength += bufferSize;
            //         recordingLength += resampled.renderedBuffer.length;
            //         console.log(resampled.renderedBuffer.length);
            //         // console.log(''+e.inputBuffer.length);
            //         // console.log(''+resampled.length);
            //     };

            //     // offlineCtx.startRendering().then((resampled) => {
            //     // // `resampled` contains an AudioBuffer resampled at 16000Hz.
            //     // // use resampled.getChannelData(x) to get an Float32Array for channel x.
            //     //     leftchannel.push(new Float32Array(resampled.getChannelData(0)));
            //     //     // rightchannel.push(new Float32Array(resampled.getChannelData(1)));    // using mono channel
            //     //     // recordingLength += bufferSize;
            //     //     recordingLength += resampled.length;
            //     //     // console.log(''+e.inputBuffer.length);
            //     //     // console.log(''+resampled.length);
            //     // });

            //     // leftchannel.push(new Float32Array(e.inputBuffer.getChannelData(0)));
            //     // rightchannel.push(new Float32Array(e.inputBuffer.getChannelData(1)));
                
            // }

            // we connect the recorder
            mediaStream.connect(recorder);
            recorder.connect(context.destination);
        }

        function stopRecording() {
            // stop recording
            recorder.disconnect(context.destination);
            mediaStream.disconnect(recorder);

            // we flat the left and right channels down
            // Float32Array[] => Float32Array
            var leftBuffer = flattenArray(leftchannel, recordingLength);
            // var rightBuffer = flattenArray(rightchannel, recordingLength);   // using only mono channel
            // we interleave both channels together
            // [left[0],right[0],left[1],right[1],...]
            // var interleaved = interleave(leftBuffer, rightBuffer);   // using only mono channel
            // interleaved = leftBuffer;
            interleaved = downSample(leftBuffer);

            // we create our wav file
            var buffer = new ArrayBuffer(44 + interleaved.length * 2);
            var view = new DataView(buffer);

            // RIFF chunk descriptor
            writeUTFBytes(view, 0, 'RIFF');
            view.setUint32(4, 44 + interleaved.length * 2, true);
            writeUTFBytes(view, 8, 'WAVE');
            // FMT sub-chunk
            writeUTFBytes(view, 12, 'fmt ');
            view.setUint32(16, 16, true); // chunkSize
            view.setUint16(20, 1, true); // wFormatTag
            view.setUint16(22, 1, true); // wChannels: stereo (2 channels)  // using only mono channel
            view.setUint32(24, TARGET_SAMPLE_RATE, true); // dwSamplesPerSec
            // view.setUint32(24, sampleRate, true); // dwSamplesPerSec
            view.setUint32(28, TARGET_SAMPLE_RATE * 4, true); // dwAvgBytesPerSec
            // view.setUint32(28, sampleRate * 4, true); // dwAvgBytesPerSec
            view.setUint16(32, 4, true); // wBlockAlign
            view.setUint16(34, 16, true); // wBitsPerSample
            // data sub-chunk
            writeUTFBytes(view, 36, 'data');
            view.setUint32(40, interleaved.length * 2, true);

            // write the PCM samples
            var index = 44;
            var volume = 1;
            for (var i = 0; i < interleaved.length; i++) {
                view.setInt16(index, interleaved[i] * (0x7FFF * volume), true);
                index += 2;
            }

            // our final blob
            blob = new Blob([view], { type: 'audio/wav' });
        }

        function getEmotion(){
            if (blob == null){
                console.warn("Blob is none");
            }
            var formData = new FormData()
            formData.append('apikey', apiKey)
            formData.append('wav', blob)

            $.ajax({
                    url: apiEndPointV2, 
                    type: 'POST',
                    data: formData,
                    processData: false,
                    contentType: false,
                    success: function(data) {
                        apiReturnData = data;
                        console.log(JSON.stringify(data));
                        
                        $('#result').show();
                        if ('error' in data && 'msg' in data){
                            $('#li_error').show();
                            $('#result_error').html(data.msg + '(code: '+data.error+')');
                        }
                        if ('calm' in data){
                            $('#result_calm').html(data.calm);
                        }
                        if ('anger' in data){
                            $('#result_anger').html(data.anger);
                        }
                        if ('joy' in data){
                            $('#result_joy').html(data.joy);
                        }
                        if ('sorrow' in data){
                            $('#result_sorrow').html(data.sorrow);
                        }
                        if ('energy' in data){
                            $('#result_energy').html(data.energy);
                        }
                    }
            });
            // var emotionQuery = $.post(apiEndPointV2, 
            //         {apikey : apiKey, 
            //          wav    : blob}, 
            //         function(data){
            //             console.log('Emotion returned: '+data);
            //         }
            // );
        }

        function flattenArray(channelBuffer, recordingLength) {
            var result = new Float32Array(recordingLength);
            var offset = 0;
            for (var i = 0; i < channelBuffer.length; i++) {
                var buffer = channelBuffer[i];
                result.set(buffer, offset);
                offset += buffer.length;
            }
            return result;
        }

        function interleave(leftChannel, rightChannel) {
            var length = leftChannel.length + rightChannel.length;
            var result = new Float32Array(length);

            var inputIndex = 0;

            for (var index = 0; index < length;) {
                result[index++] = leftChannel[inputIndex];
                result[index++] = rightChannel[inputIndex];
                inputIndex++;
            }
            return result;
        }

        function downSample(channel){   // for 44100 to 11025
            var length = channel.length;
            var result = new Float32Array(Math.trunc(length / 4));

            for (var index = 0; index < result.length;){
                result[index++] = channel[index*4];
            }
            return result;
        }
        function writeUTFBytes(view, offset, string) {
            for (var i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }



    </script>
</body>
</html>

